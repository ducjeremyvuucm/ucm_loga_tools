{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import datetime as d \n",
    "load_dotenv()\n",
    "\n",
    "# get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# change the current working directory to the parent directory\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "\n",
    "path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "USRPATH = \"/Users/ducjeremyvu/dev/\"\n",
    "\n",
    "\n",
    "sys.path.append(Path().absolute().parent)\n",
    "sys.path.append(\"/Users/ducjeremyvu/dev/wspace/nb\")\n",
    "sys.path.append(parent)\n",
    "sys.path.append(...)\n",
    "\n",
    "os.getcwd()\n",
    "import re\n",
    "\n",
    "# from g_api import gservice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "folder_path = \"/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135\"\n",
    "\n",
    "# Compile the regex pattern to find the substring between numbers_underscore and .csv\n",
    "pattern = re.compile(r'\\d+_\\d+_(.*?)\\.csv')\n",
    "\n",
    "\n",
    "# Dictionary comprehension to extract the desired part from filename\n",
    "folder_extract = {pattern.search(file.name).group(1): file \n",
    "                  for file in Path(folder_path).glob(\"*.csv\") \n",
    "                  if pattern.search(file.name)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary comprehension to read CSV files into pandas DataFrames\n",
    "csv_read = {key: pd.read_csv(path, sep=\";\", dtype=str)\n",
    "            for key, path in folder_extract.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pers_edit': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_pers_edit.csv'),\n",
       " 'PSTKST': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_PSTKST.csv'),\n",
       " 'pers': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_pers.csv'),\n",
       " 'bank': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_bank.csv'),\n",
       " 'steuer': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_steuer.csv'),\n",
       " 'sv': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_sv.csv'),\n",
       " 'contract': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_contract.csv'),\n",
       " 'danger': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_danger.csv'),\n",
       " 'danger_edit': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_danger_edit.csv'),\n",
       " 'wd': PosixPath('/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_wd.csv')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pers_edit', 'PSTKST', 'pers', 'bank', 'steuer', 'sv', 'contract', 'danger', 'danger_edit', 'wd'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dict = folder_extract\n",
    "path_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVPNR</th>\n",
       "      <th>SVVERTNR</th>\n",
       "      <th>join_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A37470</td>\n",
       "      <td>14</td>\n",
       "      <td>A37470_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A54575</td>\n",
       "      <td>35</td>\n",
       "      <td>A54575_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A56835</td>\n",
       "      <td>2</td>\n",
       "      <td>A56835_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A57565</td>\n",
       "      <td>3</td>\n",
       "      <td>A57565_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A57921</td>\n",
       "      <td>2</td>\n",
       "      <td>A57921_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A59358</td>\n",
       "      <td>1</td>\n",
       "      <td>A59358_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A60421</td>\n",
       "      <td>5</td>\n",
       "      <td>A60421_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A65525</td>\n",
       "      <td>1</td>\n",
       "      <td>A65525_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A66735</td>\n",
       "      <td>89</td>\n",
       "      <td>A66735_89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A69556</td>\n",
       "      <td>3</td>\n",
       "      <td>A69556_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A70917</td>\n",
       "      <td>3</td>\n",
       "      <td>A70917_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A71577</td>\n",
       "      <td>3</td>\n",
       "      <td>A71577_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A71716</td>\n",
       "      <td>4</td>\n",
       "      <td>A71716_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A71822</td>\n",
       "      <td>1</td>\n",
       "      <td>A71822_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A72302</td>\n",
       "      <td>2</td>\n",
       "      <td>A72302_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A72711</td>\n",
       "      <td>3</td>\n",
       "      <td>A72711_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A72760</td>\n",
       "      <td>20</td>\n",
       "      <td>A72760_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A73105</td>\n",
       "      <td>3</td>\n",
       "      <td>A73105_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A76563</td>\n",
       "      <td>46</td>\n",
       "      <td>A76563_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A77941</td>\n",
       "      <td>1</td>\n",
       "      <td>A77941_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A78034</td>\n",
       "      <td>6</td>\n",
       "      <td>A78034_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A78043</td>\n",
       "      <td>5</td>\n",
       "      <td>A78043_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A78364</td>\n",
       "      <td>25</td>\n",
       "      <td>A78364_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A78518</td>\n",
       "      <td>7</td>\n",
       "      <td>A78518_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A78627</td>\n",
       "      <td>4</td>\n",
       "      <td>A78627_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A78628</td>\n",
       "      <td>4</td>\n",
       "      <td>A78628_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A78999</td>\n",
       "      <td>7</td>\n",
       "      <td>A78999_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A79103</td>\n",
       "      <td>30</td>\n",
       "      <td>A79103_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A79246</td>\n",
       "      <td>14</td>\n",
       "      <td>A79246_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A79280</td>\n",
       "      <td>5</td>\n",
       "      <td>A79280_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A79856</td>\n",
       "      <td>29</td>\n",
       "      <td>A79856_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A80005</td>\n",
       "      <td>82</td>\n",
       "      <td>A80005_82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A80274</td>\n",
       "      <td>1</td>\n",
       "      <td>A80274_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A80601</td>\n",
       "      <td>1</td>\n",
       "      <td>A80601_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A80738</td>\n",
       "      <td>1</td>\n",
       "      <td>A80738_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A81030</td>\n",
       "      <td>35</td>\n",
       "      <td>A81030_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>A81371</td>\n",
       "      <td>1</td>\n",
       "      <td>A81371_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>A81413</td>\n",
       "      <td>3</td>\n",
       "      <td>A81413_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A81821</td>\n",
       "      <td>187</td>\n",
       "      <td>A81821_187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A82007</td>\n",
       "      <td>1</td>\n",
       "      <td>A82007_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>A82615</td>\n",
       "      <td>1</td>\n",
       "      <td>A82615_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>A82851</td>\n",
       "      <td>1</td>\n",
       "      <td>A82851_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A84256</td>\n",
       "      <td>1</td>\n",
       "      <td>A84256_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SVPNR SVVERTNR    join_key\n",
       "0   A37470       14   A37470_14\n",
       "1   A54575       35   A54575_35\n",
       "2   A56835        2    A56835_2\n",
       "3   A57565        3    A57565_3\n",
       "4   A57921        2    A57921_2\n",
       "5   A59358        1    A59358_1\n",
       "6   A60421        5    A60421_5\n",
       "7   A65525        1    A65525_1\n",
       "8   A66735       89   A66735_89\n",
       "9   A69556        3    A69556_3\n",
       "10  A70917        3    A70917_3\n",
       "11  A71577        3    A71577_3\n",
       "12  A71716        4    A71716_4\n",
       "13  A71822        1    A71822_1\n",
       "14  A72302        2    A72302_2\n",
       "15  A72711        3    A72711_3\n",
       "16  A72760       20   A72760_20\n",
       "17  A73105        3    A73105_3\n",
       "18  A76563       46   A76563_46\n",
       "19  A77941        1    A77941_1\n",
       "20  A78034        6    A78034_6\n",
       "21  A78043        5    A78043_5\n",
       "22  A78364       25   A78364_25\n",
       "23  A78518        7    A78518_7\n",
       "24  A78627        4    A78627_4\n",
       "25  A78628        4    A78628_4\n",
       "26  A78999        7    A78999_7\n",
       "27  A79103       30   A79103_30\n",
       "28  A79246       14   A79246_14\n",
       "29  A79280        5    A79280_5\n",
       "30  A79856       29   A79856_29\n",
       "31  A80005       82   A80005_82\n",
       "32  A80274        1    A80274_1\n",
       "33  A80601        1    A80601_1\n",
       "34  A80738        1    A80738_1\n",
       "35  A81030       35   A81030_35\n",
       "36  A81371        1    A81371_1\n",
       "37  A81413        3    A81413_3\n",
       "38  A81821      187  A81821_187\n",
       "39  A82007        1    A82007_1\n",
       "40  A82615        1    A82615_1\n",
       "41  A82851        1    A82851_1\n",
       "42  A84256        1    A84256_1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = pd.read_csv(path_dict.get('sv'),dtype=str, sep=\";\")\n",
    "remove = sv[sv[\"SVPSCHL\"] == \"106\"].groupby([\"SVPNR\",\"SVVERTNR\"])[\"MASK\"].count().reset_index()[[\"SVPNR\",\"SVVERTNR\"]]\n",
    "remove[\"join_key\"] = remove[\"SVPNR\"] + \"_\" + remove[\"SVVERTNR\"]\n",
    "remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ducjeremyvu/Downloads/ucmdownload/PR_202411_135/PR_202411_135_sv.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(folder_extract.get(\"sv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PR_202411_135_'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import PosixPath\n",
    "import re\n",
    "\n",
    "\n",
    "pattern = \"PR_\\d{6}_\\d{3}_\"\n",
    "\n",
    "def extract_from_regex(pattern, path):\n",
    "    if isinstance(path, PosixPath):\n",
    "        path = str(path)\n",
    "    matches = re.findall(pattern, path)\n",
    "    return matches\n",
    "str_1 = \"PR_202405_123_contract\"\n",
    "\n",
    "\n",
    "stem = extract_from_regex(pattern, folder_extract.get(\"sv\")\n",
    ")[0]\n",
    "\n",
    "stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A37470',\n",
       " 'A54575',\n",
       " 'A56835',\n",
       " 'A57565',\n",
       " 'A57921',\n",
       " 'A59358',\n",
       " 'A60421',\n",
       " 'A65525',\n",
       " 'A66735',\n",
       " 'A69556',\n",
       " 'A70917',\n",
       " 'A71577',\n",
       " 'A71716',\n",
       " 'A71822',\n",
       " 'A72302',\n",
       " 'A72711',\n",
       " 'A72760',\n",
       " 'A73105',\n",
       " 'A76563',\n",
       " 'A77941',\n",
       " 'A78034',\n",
       " 'A78043',\n",
       " 'A78364',\n",
       " 'A78518',\n",
       " 'A78627',\n",
       " 'A78628',\n",
       " 'A78999',\n",
       " 'A79103',\n",
       " 'A79246',\n",
       " 'A79280',\n",
       " 'A79856',\n",
       " 'A80005',\n",
       " 'A80274',\n",
       " 'A80601',\n",
       " 'A80738',\n",
       " 'A81030',\n",
       " 'A81371',\n",
       " 'A81413',\n",
       " 'A81821',\n",
       " 'A82007',\n",
       " 'A82615',\n",
       " 'A82851',\n",
       " 'A84256']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "remove[\"SVPNR\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steuer = csv_read.get('steuer')\n",
    "steuer_cols = steuer.columns \n",
    "\n",
    "\n",
    "steuer.groupby(['STPNR', 'STVERTNR']).agg({\"STEL2AN\":\"min\",\"STEL2AB\":\"max\",'MASK':\"max\", 'MODUS':\"max\", 'STMAN':\"max\", 'STAK':\"max\",'STLSTTAB':\"max\",\n",
    "       'STNEBAG':\"max\", 'STSCHL':\"max\", 'STKISTAN':\"max\", 'STKISTEG':\"max\", 'STBIMS':\"max\", 'STEL2ANK':\"max\",\n",
    "       'GUELTIG_AB':\"max\"}).reset_index()[steuer_cols].to_csv(folder_path+f\"/{stem}steuer_edit_grouped.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pers_edit', 'PSTKST', 'pers', 'bank', 'steuer', 'sv', 'contract', 'danger', 'steuer_edit_grouped', 'danger_edit', 'wd'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compile the regex pattern to find the substring between numbers_underscore and .csv\n",
    "pattern = re.compile(r'\\d+_\\d+_(.*?)\\.csv')\n",
    "\n",
    "\n",
    "# Dictionary comprehension to extract the desired part from filename\n",
    "folder_extract = {pattern.search(file.name).group(1): file \n",
    "                  for file in Path(folder_path).glob(\"*.csv\") \n",
    "                  if pattern.search(file.name)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary comprehension to read CSV files into pandas DataFrames\n",
    "csv_read = {key: pd.read_csv(path, sep=\";\", dtype=str)\n",
    "            for key, path in folder_extract.items()}\n",
    "\n",
    "\n",
    "path_dict = folder_extract\n",
    "path_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "file_path = (\n",
    "    path_dict.get('steuer_edit_grouped')\n",
    ")\n",
    "\n",
    "file_name_part = file_path.stem.split(\"/\")[-1].split(\"_\")\n",
    "\n",
    "# Read initial CSV\n",
    "input_data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\";\",\n",
    "    dtype=str,\n",
    ")\n",
    "input_data[\"STVERTNR\"] = input_data[\"STVERTNR\"].astype(int)\n",
    "\n",
    "input_data_lt = input_data[(input_data[\"STPNR\"].astype(str) + \"_\" + input_data[\"STVERTNR\"].astype(str)).isin(remove[\"join_key\"].to_list())]\n",
    "\n",
    "input_data = input_data[~(input_data[\"STPNR\"].astype(str) + \"_\" + input_data[\"STVERTNR\"].astype(str)).isin(remove[\"join_key\"].to_list())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PR', '202411', '135', 'steuer', 'edit', 'grouped']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_name_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subfolder based on the first parts of the file name\n",
    "subfolder = f\"./out/elstam/{file_name_part[1]}_{file_name_part[2]}\"\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "input_data_lt[\"STEL2ANK\"] = \"0\"\n",
    "input_data_lt.to_csv(\n",
    "        f\"{subfolder}/{file_name_part[1]}_{file_name_part[2]}_LT_signin.csv\",\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drop unnecessary columns from original DataFrame\n",
    "clean_data = input_data.drop(\n",
    "    columns=[\"STLSTTAB\", \"STSCHL\", \"STKISTAN\", \"STKISTEG\", \"STBIMS\"]\n",
    ")\n",
    "\n",
    "# Initialize remaining data DataFrame\n",
    "remaining_data = input_data.copy()\n",
    "\n",
    "# Create subfolder based on the first parts of the file name\n",
    "subfolder = f\"./out/elstam/{file_name_part[1]}_{file_name_part[2]}\"\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "# Initialize counter for batch\n",
    "batch_counter = 1\n",
    "\n",
    "# creates a dictionary for later use when grouping, this dictonary is used in the agg function and keeps all columns that were not used for aggregation but are relevan for the import \n",
    "keep_dict = { el: \"first\" for el in clean_data.columns if el not in [\"STPNR\", \"STVERTNR\", \"STEL2AN\", \"STEL2AB\"] }\n",
    "\n",
    "# the update ensures the aggreation for start and end date---\n",
    "keep_dict.update({\"STEL2AN\": \"min\", \"STEL2AB\": \"max\"}\n",
    "                 )\n",
    "# Loop until remaining_data is empty\n",
    "while not remaining_data.empty:\n",
    "    # Group by and find min value for each group\n",
    "    group_min_values = remaining_data.groupby(\"STPNR\")[\"STVERTNR\"].min().reset_index()\n",
    "\n",
    "    # Merge with another DataFrame\n",
    "    merged_data = (\n",
    "        group_min_values.merge(clean_data, on=[\"STPNR\", \"STVERTNR\"], how=\"left\")[\n",
    "            clean_data.columns\n",
    "        ]\n",
    "        .groupby([\"STPNR\", \"STVERTNR\"])\n",
    "        .agg(keep_dict)\n",
    "        .reset_index()\n",
    "    )[clean_data.columns]\n",
    "\n",
    "    # Update and save CSV files\n",
    "    merged_data[\"STEL2ANK\"] = \"0\"\n",
    "    merged_data.to_csv(\n",
    "        f\"{subfolder}/{file_name_part[1]}_{file_name_part[2]}_batch_{batch_counter}_signin.csv\",\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "    merged_data[\"STEL2ANK\"] = \"3\"\n",
    "    merged_data.to_csv(\n",
    "        f\"{subfolder}/{file_name_part[1]}_{file_name_part[2]}_batch_{batch_counter}_signoff.csv\",\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    # Update remaining data\n",
    "    remaining_ids = group_min_values[\"STPNR\"] + group_min_values[\"STVERTNR\"].astype(str)\n",
    "    remaining_data = remaining_data.loc[\n",
    "        ~(remaining_data[\"STPNR\"] + remaining_data[\"STVERTNR\"].astype(str)).isin(\n",
    "            remaining_ids\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Update the batch counter\n",
    "    batch_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MKII -> does not not incorporate multiple rows per vertnr\n",
    "\n",
    "# import pandas as pd\n",
    "# import os \n",
    "\n",
    "# file_path = \"/Users/ducjeremyvu/Downloads/ucmdownload/PR_202310_104/PR_202310_104_steuer.csv\"\n",
    "# file_name_part = file_path.split(\"/\")[-1].split(\"_\")\n",
    "\n",
    "# # Read initial CSV\n",
    "# input_data = pd.read_csv(\n",
    "#     file_path,\n",
    "#     sep=\";\",\n",
    "#     dtype=str,\n",
    "# )\n",
    "# input_data[\"STVERTNR\"] = input_data[\"STVERTNR\"].astype(int)\n",
    "\n",
    "# # Drop unnecessary columns from original DataFrame\n",
    "# clean_data = input_data.drop(columns=[\"STLSTTAB\", \"STSCHL\", \"STKISTAN\", \"STKISTEG\", \"STBIMS\"]) \n",
    "\n",
    "# # Initialize remaining data DataFrame\n",
    "# remaining_data = input_data.copy()\n",
    "\n",
    "# # Create subfolder based on the first parts of the file name\n",
    "# subfolder = f\"./out/elstam/{file_name_part[1]}_{file_name_part[2]}\"\n",
    "# if not os.path.exists(subfolder):  \n",
    "#     os.makedirs(subfolder)  \n",
    "\n",
    "# # Initialize counter for batch\n",
    "# batch_counter = 1\n",
    "\n",
    "# # Initialize a variable to hold the sum of processed rows\n",
    "# total_processed_rows = 0\n",
    "\n",
    "# # Loop until condition is met\n",
    "# while True:\n",
    "#     # Group by and find min value for each group\n",
    "#     group_min_values = remaining_data.groupby(\"STPNR\")[\"STVERTNR\"].min().reset_index()\n",
    "    \n",
    "#     # Merge with another DataFrame\n",
    "#     merged_data = group_min_values.merge(clean_data, on=[\"STPNR\", \"STVERTNR\"], how=\"left\")[clean_data.columns]\n",
    "    \n",
    "#     # Update and save CSV files\n",
    "#     merged_data[\"STEL2ANK\"] = \"0\"\n",
    "#     merged_data.to_csv(f\"{subfolder}/{file_name_part[1]}_{file_name_part[2]}_batch_{batch_counter}_signin_old.csv\", index=False, sep=\";\")\n",
    "#     merged_data[\"STEL2ANK\"] = \"3\"\n",
    "#     merged_data.to_csv(f\"{subfolder}/{file_name_part[1]}_{file_name_part[2]}_batch_{batch_counter}_signoff_old.csv\", index=False, sep=\";\")\n",
    "    \n",
    "#     # Update remaining data\n",
    "#     remaining_ids = group_min_values[\"STPNR\"] + group_min_values[\"STVERTNR\"].astype(str)\n",
    "#     remaining_data = remaining_data.loc[\n",
    "#         ~(remaining_data[\"STPNR\"] + remaining_data[\"STVERTNR\"].astype(str)).isin(remaining_ids)\n",
    "#     ]\n",
    "    \n",
    "#     # Update the total_processed_rows\n",
    "#     total_processed_rows += len(merged_data)\n",
    "    \n",
    "#     # Update the batch counter\n",
    "#     batch_counter += 1\n",
    "    \n",
    "#     # Break condition: Exit loop if all elements have been processed\n",
    "#     if total_processed_rows >= input_data.shape[0]:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
